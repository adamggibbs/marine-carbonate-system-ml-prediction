{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Estimations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPf0kJd0AXrG/oaV3V6xj2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamggibbs/marine-carbonate-system-ml-prediction/blob/master/Model_Estimations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Estimation\n",
        "\n",
        "This notebook uses a defined TensorFlow model to estimate a specified output variable from specified input variables. The notebook will take in an arbitrary number of data files, make estimations, and add the estimations to the data files as a new column, and save a new data file with the estimations.\n",
        "\n",
        "**Directions:**\n",
        "\n",
        "1. Place data files in the `to_estimate` directory.\n",
        "2. Define the necessary user defined varibales including model name, model file path, input and output variables, type of input file, and desired type of output file.\n",
        "3. Run the notebook by going to `Runtime -> Run all` or using `ctrl+F9`. All output files with be saved in the `estimations` directory.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "chDzazV5Vmf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Set up environment.\n",
        "# SET UP ENVIRONMENT \n",
        "\n",
        "# mount google drive for data storage and access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# IMPORTS\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "metadata": {
        "id": "BQUnynejj03h",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ba049e-6312-44b7-ecca-6aa4c01ecba4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Necessary User Defined Variables"
      ],
      "metadata": {
        "id": "wJmNxr6ncysV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# name of folder in Google Drive \n",
        "# should be a relative path from 'My Drive' and end with trailing '/'\n",
        "# exclude beginning '/'\n",
        "root_dir_name = 'Example/'\n",
        "\n",
        "# desired name of trained model\n",
        "model_name = 'model_name'\n",
        "model_file_name = 'model_name_Layers(48, 24)'\n",
        "\n",
        "input_vars = ['DATE', 'LATITUDE', 'LONGITUDE', 'PRS', 'TMP', 'SAL', 'OXYGEN']\n",
        "output_var = 'PH_INSITU'\n",
        "\n",
        "# what file type you're using,\n",
        "# 'csv' for any comma separated value, 'txt' for any tab separated value\n",
        "input_file_type = 'csv'\n",
        "\n",
        "# save options for intermediate data files\n",
        "# can save both or either .txt and .csv files\n",
        "save_txt = True\n",
        "save_csv = True\n"
      ],
      "metadata": {
        "id": "B9v672wQlkgp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Initialize directory variables.\n",
        "# data dirs\n",
        "root_dir = '/content/drive/MyDrive/'+ root_dir_name\n",
        "data_dir = root_dir + 'data/'\n",
        "est_dir = data_dir + 'to_estimate/'\n",
        "est_results_dir = data_dir + 'estimations/'\n",
        "model_dir = root_dir + 'models/'\n",
        "fig_dir = root_dir + 'figs/'\n",
        "\n",
        "model_path = model_dir + model_file_name"
      ],
      "metadata": {
        "id": "BSs_xg2JlUdm",
        "cellView": "form"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Initialize data cleaning preprocessing functions.\n",
        "# FUNCTION TO READ GLIDER FILE IN A PANDAS DATAFRAME\n",
        "def read_data_file(file):\n",
        "  names = [ 'Cruise', 'Station', 'Type',\t'DATE', 'TIME', 'LONGITUDE',\t\n",
        "         'LATITUDE',\t'QF',\t'PRS', 'PRS_QF', 'TMP', 'TMP_QF', \n",
        "         'SAL', 'SAL_QF', 'Sigma_theta', 'ST_QF', 'DEPTH', 'DEPTH_QF', \n",
        "         'OXYGEN', 'OXYGEN_QF',\t'SATOXY',\t'SATOXY_QF',\t'NITRATE', \n",
        "         'NITRATE_QF', 'CHL_A', 'CHL_A_QF', 'BBP700', 'BBP700_QF', 'PH_INSITU', \n",
        "         'PH_INSITU_QF', 'BBP532', 'BBP_532_QF', 'CDOM', 'CDOM_QF', 'TALK_CANYONB',\t\n",
        "         'TALK_QF', 'DIC_CANYONB', 'DIC_QF', 'pCO2_CANYONB', 'pCO2_QF', \n",
        "         'SAT_AR_CANYONB', 'SAT_AR_QF', 'pH25C_1atm', 'pH25C_1atm_QF' ]\n",
        "\n",
        "  if input_file_type == 'csv':\n",
        "    df = pd.read_csv(file, header=0, sep=',')\n",
        "  else:\n",
        "    df = pd.read_csv(file, skiprows=7, header=None, sep='\\t', names=names)\n",
        "  \n",
        "  df = df.dropna(axis=0, how='any')\n",
        "  \n",
        "  return df\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# CREATE FUNCTION TO CREATE A NUMPY ARRAY OF INPUTS FROM\n",
        "# GLIDER DATA FILE\n",
        "\n",
        "'''\n",
        "process_data_file()\n",
        "  description:\n",
        "    This function reads in a data file in csv format and\n",
        "    creates a pandas dataframe from it. From there it loops through\n",
        "    and removes all bad data points according to the quality control\n",
        "    flags. It then takes the desired input parameters as sepcified on \n",
        "    line 84 and puts them into a numpy array.\n",
        "\n",
        "  args:\n",
        "    file: string that contains file name of dataset\n",
        "'''\n",
        "def process_data_file(file, save_txt=False, save_csv=False):\n",
        "\n",
        "  df = read_data_file(file)\n",
        "\n",
        "  # decide data we care about\n",
        "  relevant_vars = []\n",
        "  # add input vars and their QFs\n",
        "  for var in input_vars:\n",
        "    relevant_vars.append(var)\n",
        "    if var not in ['DATE', 'LATITUDE', 'LONGITUDE']:\n",
        "      relevant_vars.append(var + '_QF')\n",
        "  # take only data we care about\n",
        "  df = df[relevant_vars]\n",
        "\n",
        "  for input_var in input_vars:\n",
        "    if input_var in ['DATE', 'LATITUDE', 'LONGITUDE']:\n",
        "      continue\n",
        "    # drop bad inputs\n",
        "    to_drop = []\n",
        "    for index, row in df.iterrows():\n",
        "      if int(row[input_var + '_QF']) > 0:\n",
        "        to_drop.append(index)\n",
        "    df = df.drop(to_drop)\n",
        "    # index = 0\n",
        "    # to_drop = []\n",
        "    # for flag in df[input_var + '_QF']:\n",
        "    #   if int(flag) > 0:\n",
        "    #     to_drop.append(index)\n",
        "    #   index += 1\n",
        "    # df = df.drop(to_drop).reset_index(drop=True)\n",
        "\n",
        "  # take subset of only parameters for inputs\n",
        "  # this array contains only \"good\" data points\n",
        "  inputs = df[input_vars]\n",
        "  # convert dataframe in numpy array\n",
        "  indices = df.index.to_numpy()\n",
        "  inputs = inputs.to_numpy(dtype='str')\n",
        "  # inputs = np.concatenate((indices.T, inputs), axis=1)\n",
        "  # change date format\n",
        "  for row in inputs:\n",
        "    date = row[0]\n",
        "    row[0] = date[6:10] + date[0:2] + date[3:5]\n",
        "\n",
        "  # return the array\n",
        "  return inputs, indices\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def prep_data(inputs):\n",
        "\n",
        "  # TRANSFORM DATE AND PRESSURE INPUTS\n",
        "\n",
        "  # method to help transform date\n",
        "  def date_to_nth_day(the_date):\n",
        "    date = pd.to_datetime(the_date)\n",
        "    new_year_day = pd.Timestamp(year=date.year, month=1, day=1)\n",
        "    day_of_the_year = (date - new_year_day).days + 1\n",
        "    return day_of_the_year\n",
        "\n",
        "  # loop through inputs and perform transformations\n",
        "  for input in inputs:\n",
        "    # adjust date\n",
        "    date = input[0]\n",
        "    frac_year = date_to_nth_day(date) / 365.0\n",
        "    input[0] = int(date[0:4]) + frac_year \n",
        "    \n",
        "  return inputs"
      ],
      "metadata": {
        "id": "keq8RwJ0kyTl",
        "cellView": "form"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Get input data.\n",
        "input_arrays = []\n",
        "indices_arrays = []\n",
        "\n",
        "for file in os.listdir(est_dir):\n",
        "  print(\"Processing \" + est_dir + file + \" ... \", end=\"\")\n",
        "\n",
        "  curr_inputs, indices = process_data_file(est_dir+file)\n",
        "  indices_arrays.append(indices)\n",
        "  curr_inputs = prep_data(curr_inputs)\n",
        "  input_arrays.append(curr_inputs)\n",
        "\n",
        "  print(\"Complete.\")\n"
      ],
      "metadata": {
        "id": "yHI5nE59kQd3",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65099f52-a166-43e9-977f-69e234d0e6ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/MyDrive/Example/data/to_estimate/19A02901.csv ... Complete.\n",
            "Processing /content/drive/MyDrive/Example/data/to_estimate/19502902.csv ... Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Load in model.\n",
        "# LOAD IN MODEL\n",
        "\n",
        "print(\"Loading in model... \", end=\"\")\n",
        "model = tf.keras.models.load_model(model_path, compile=True)\n",
        "print(\"Complete.\\nModel Summary:\\n\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "-2EkXYKJl3QU",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afdf1917-6d1a-4334-a391-6701822a3344"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading in model... Complete.\n",
            "Model Summary:\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_2 (Normalizat  (None, 7)                15        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 48)                384       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 24)                1176      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,600\n",
            "Trainable params: 1,585\n",
            "Non-trainable params: 15\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Make estimations and save to data files.\n",
        "\n",
        "input_array = 0\n",
        "indices_array = 0\n",
        "for file in os.listdir(est_dir):\n",
        "  print(\"Loading \" + est_dir + file + \" ... \", end=\"\")\n",
        "  df = read_data_file(est_dir+file)\n",
        "  print(\"Complete.\")\n",
        "\n",
        "  curr_inputs = input_arrays[input_array].astype('float')\n",
        "\n",
        "  print(\"Making estimations for \" + output_var + \" ... \")\n",
        "  predictions = model.predict(curr_inputs, verbose=1).flatten()\n",
        "  print(\"Complete.\")\n",
        "\n",
        "  print(\"Adding estimations to data file... \",end=\"\")\n",
        "  preds = []\n",
        "  input_inds = 0\n",
        "  for index, row in df.iterrows():\n",
        "    if index == indices_arrays[indices_array][input_inds]:\n",
        "      preds.append(predictions[input_inds])\n",
        "      input_inds += 1\n",
        "    else:\n",
        "      preds.append(np.nan)\n",
        "\n",
        "  preds = np.array(preds)\n",
        "  df[model_name + '_' + output_var] = preds\n",
        "  print(\"Complete.\")\n",
        "\n",
        "  print(\"Saving file... \", end=\"\")\n",
        "  if save_csv:\n",
        "    df.to_csv(est_results_dir + 'Estimated ' + file[:-4] + '.csv', index=False)\n",
        "  if save_txt:\n",
        "    df.to_csv(est_results_dir + 'Estimated ' + file[:-4] + '.txt', index=False, sep='\\t')\n",
        "  print(\"Complete.\\n\")\n",
        "\n",
        "  input_array += 1\n",
        "  indices_array += 1\n",
        "\n",
        "print(\"\\nAll files processed.\")"
      ],
      "metadata": {
        "id": "7Cg9LxZ8GHUT",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865bdabf-92e7-4c74-aff8-5fb1bc4abff1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/drive/MyDrive/Example/data/to_estimate/19A02901.csv ... Complete.\n",
            "Making estimations for PH_INSITU ... \n",
            "2058/2058 [==============================] - 2s 1ms/step\n",
            "Complete.\n",
            "Adding estimations to data file... Complete.\n",
            "Saving file... Complete.\n",
            "\n",
            "Loading /content/drive/MyDrive/Example/data/to_estimate/19502902.csv ... Complete.\n",
            "Making estimations for PH_INSITU ... \n",
            "1647/1647 [==============================] - 2s 1ms/step\n",
            "Complete.\n",
            "Adding estimations to data file... Complete.\n",
            "Saving file... Complete.\n",
            "\n",
            "\n",
            "All files processed.\n"
          ]
        }
      ]
    }
  ]
}
