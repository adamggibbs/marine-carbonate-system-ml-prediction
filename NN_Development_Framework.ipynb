{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_Development_Framework.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rr6dpfJ_jpon",
        "AK1lP-VCegLf",
        "gvaakTl9eUyX",
        "EpzRrZYY7P-Z"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamggibbs/marine-carbonate-system-ml-prediction/blob/master/NN_Development_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6CB5CrwBScs"
      },
      "source": [
        "# ML Model Training Pipeline\n",
        "This notebook will train a neural network to estimate pH given time, location, temperature, pressure, salinity, oxygen as inputs. This notebook automates the entire pipeline including quality control, data cleaning, data preprocessing, model training, and model evaluation. \n",
        "\n",
        "Before running this notebook:\n",
        "1. Create a folder in your Google Drive to store all data and models\n",
        "2. Run the Directory Setup Colab notebook to create the necessary directory structure for this notebook to run.\n",
        "3. Place all training files you want to use for training in the 'training/' directory\n",
        "4. Place all testing files you want to use for testing in the 'testing/' directory\n",
        "5. Check User Defined variables at the top of this notebook\n",
        "\n",
        "Once all these steps have been satisfied, go to the 'Runtime' tab and select 'Run All' or click ctrl+f9 to run the entire notebook. This notebook will take approximately **1 hour to run and fully complete** with this time varying based on the number of training and testing files as well as the size of the neural network you choose.\n",
        "\n",
        "When complete this notebook will have trained and saved a neural network model in the 'models/' directory for you to use to estimate pH as described before. There is a Estimation notebook that can be used to input models and take in data and output the same input data but with pH estimations added. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxwj9jGPnxqt",
        "cellView": "form"
      },
      "source": [
        "#@title # Set up environment.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "sns.set_color_codes(palette='colorblind')\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eSEG1zDy1UR"
      },
      "source": [
        "# Necessary User Defined Variables\n",
        "Specify the user defined variables as followed:\n",
        "- `root_dir_name`  Name of the folder in Google Drive to store everything\n",
        "- `model_name`  Name of the model for figures\n",
        "- `input_vars` The input variables to be used (see vars_README.md for list)\n",
        "- `output_var` The target variable to be estimated (see vars_README.md for list)\n",
        "- `save_txt ` Boolean whether to save copies of intermediate files as .txt files\n",
        "- `save_csv`  Boolean whether to save copies of intermediate files as .csv files\n",
        "- `save_figs`  Boolean whether to save evaluation figures as own files (they will also be saved in notebook, but will be lost if you reset the notebook)\n",
        "- `show_figs`  Boolean whether to show the figures produced in the notebook output as they are made\n",
        "\n",
        "** All values filled in are basic defaults"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWTpb4lYyVBM"
      },
      "source": [
        "# name of folder in Google Drive \n",
        "# should be a relative path from 'My Drive' and end with trailing '/'\n",
        "# exclude beginning '/'\n",
        "root_dir_name = 'Example/'\n",
        "\n",
        "# desired name of trained model\n",
        "model_name = 'model_name'\n",
        "\n",
        "input_vars = ['DATE', 'LATITUDE', 'LONGITUDE', 'PRS', 'TMP', 'SAL', 'OXYGEN']\n",
        "output_var = 'PH_INSITU'\n",
        "\n",
        "# what file type you're using,\n",
        "# 'csv' for any comma separated value, 'txt' for any tab separated value\n",
        "input_file_type = 'csv'\n",
        "\n",
        "# save options for intermediate data files\n",
        "# can save both or either .txt and .csv files\n",
        "save_txt = False\n",
        "save_csv = True\n",
        "\n",
        "# display figures?\n",
        "show_figs = True\n",
        "# save figures?\n",
        "save_figs = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other User Defined Variables"
      ],
      "metadata": {
        "id": "rr6dpfJ_jpon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce density (by)\n",
        "reduce_density = False\n",
        "density_reduction_factor = 10\n",
        "\n",
        "# model hyperparams\n",
        "# list of the hidden layers as the number of neurons they have\n",
        "model_layers = [48,48]\n",
        "# list of activation functions of each layer\n",
        "# if empty, all will be set to 'sigmoid'\n",
        "# lenght of act_funcs must match length of model_layers\n",
        "act_funcs = []\n",
        "\n",
        "# model evaluation and fig settings\n",
        "shallow = 200\n",
        "d_label = \"\\n(Depths <{})\".format(shallow)\n",
        "m_col = 'b'\n",
        "c_col = 'g'"
      ],
      "metadata": {
        "id": "d0jQb4Djjmak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Pipeline"
      ],
      "metadata": {
        "id": "-pWlsIaPs0JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Initialize directory variables.\n",
        "# data dirs\n",
        "root_dir = '/content/drive/MyDrive/'+ root_dir_name\n",
        "data_dir = root_dir + 'data/'\n",
        "training_dir=data_dir + 'training/'\n",
        "testing_dir = data_dir + 'testing/'\n",
        "model_dir = root_dir + 'models/'\n",
        "fig_dir = root_dir + 'figs/'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sG1QOdVtpHXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpoV5ebp_C6g",
        "cellView": "form"
      },
      "source": [
        "#@title ### Initialize data cleaning preprocessing functions.\n",
        "# FUNCTION TO READ GLIDER FILE IN A PANDAS DATAFRAME\n",
        "def read_glider_file(file):\n",
        "  names = [ 'Cruise', 'Station', 'Type',\t'DATE', 'TIME', 'LONGITUDE',\t\n",
        "         'LATITUDE',\t'QF',\t'PRS', 'PRS_QF', 'TMP', 'TMP_QF', \n",
        "         'SAL', 'SAL_QF', 'Sigma_theta', 'ST_QF', 'DEPTH', 'DEPTH_QF', \n",
        "         'OXYGEN', 'OXYGEN_QF',\t'SATOXY',\t'SATOXY_QF',\t'NITRATE', \n",
        "         'NITRATE_QF', 'CHL_A', 'CHL_A_QF', 'BBP700', 'BBP700_QF', 'PH_INSITU', \n",
        "         'PH_INSITU_QF', 'BBP532', 'BBP_532_QF', 'CDOM', 'CDOM_QF', 'TALK_CANYONB',\t\n",
        "         'TALK_QF', 'DIC_CANYONB', 'DIC_QF', 'pCO2_CANYONB', 'pCO2_QF', \n",
        "         'SAT_AR_CANYONB', 'SAT_AR_QF', 'pH25C_1atm', 'pH25C_1atm_QF' ]\n",
        "\n",
        "  if input_file_type == 'csv':\n",
        "    df = pd.read_csv(file, header=0, sep=',')\n",
        "  else:\n",
        "    df = pd.read_csv(file, skiprows=7, header=None, sep='\\t', names=names)\n",
        "  \n",
        "  df = df.dropna(axis=0, how='any').reset_index(drop=True)\n",
        "  \n",
        "  return df\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# CREATE FUNCTION TO CREATE A NUMPY ARRAY OF INPUTS FROM\n",
        "# GLIDER DATA FILE\n",
        "\n",
        "'''\n",
        "process_glider_input()\n",
        "  description:\n",
        "    This function reads in a data file in csv format and\n",
        "    creates a pandas dataframe from it. From there it loops through\n",
        "    and removes all bad data points according to the quality control\n",
        "    flags. It then takes the desired input parameters as sepcified on \n",
        "    line 84 and puts them into a numpy array.\n",
        "\n",
        "  args:\n",
        "    file: string that contains file name of dataset\n",
        "'''\n",
        "def process_glider_file(file, save_txt=False, save_csv=False):\n",
        "\n",
        "  df = read_glider_file(file)\n",
        "\n",
        "  # throw away first day\n",
        "  start_date = int(df['DATE'][0][3:5])\n",
        "  start_time = float(df['TIME'][0][0:2]) + float(df['TIME'][0][3:5]) / 60\n",
        "\n",
        "  drop_index = 0\n",
        "  for index, row in df.iterrows():\n",
        "    curr_date = int(row['DATE'][3:5])\n",
        "    curr_time = float(row['TIME'][0:2]) + float(row['TIME'][3:5]) / 60\n",
        "    if (curr_date > start_date and curr_time > start_time) or curr_date > start_date + 1:\n",
        "      drop_index = index\n",
        "      break\n",
        "\n",
        "  # drop first day of data\n",
        "  df = df.drop(index=df.index[:drop_index], axis=0).reset_index(drop=True)\n",
        "\n",
        "  # take only data we care about\n",
        "  df = df[['DATE', 'LATITUDE', 'LONGITUDE', 'PRS', 'PRS_QF', 'TMP', 'TMP_QF',\n",
        "          'SAL', 'SAL_QF', 'OXYGEN', 'OXYGEN_QF', 'SATOXY', 'SATOXY_QF',\n",
        "          'PH_INSITU', 'PH_INSITU_QF', 'TALK_CANYONB', 'TALK_QF', 'DIC_CANYONB',\n",
        "          'DIC_QF', 'pCO2_CANYONB', 'pCO2_QF']]\n",
        "\n",
        "  for input_var in input_vars:\n",
        "      if input_var in ['DATE', 'LATITUDE', 'LONGITUDE']:\n",
        "        continue\n",
        "      # drop bad inputs\n",
        "      index = 0\n",
        "      to_drop = []\n",
        "      for flag in df[input_var + '_QF']:\n",
        "        if int(flag) > 0:\n",
        "          to_drop.append(index)\n",
        "        index += 1\n",
        "      df = df.drop(to_drop).reset_index(drop=True)\n",
        "\n",
        "  # drop bad outputs\n",
        "  index = 0\n",
        "  to_drop = []\n",
        "  for flag in df[output_var + '_QF']:\n",
        "    if int(flag) > 0:\n",
        "      to_drop.append(index)\n",
        "    index += 1\n",
        "  df = df.drop(to_drop).reset_index(drop=True)\n",
        "\n",
        "  # take subset of only parameters for inputs\n",
        "  # this array contains only \"good\" data points\n",
        "  inputs = df[input_vars]\n",
        "  outputs = df[output_var]\n",
        "  # convert dataframe in numpy array\n",
        "  inputs = inputs.to_numpy(dtype='str')\n",
        "  outputs = outputs.to_numpy(dtype='str')\n",
        "\n",
        "  # change date format\n",
        "  for row in inputs:\n",
        "    date = row[0]\n",
        "    row[0] = date[6:10] + date[0:2] + date[3:5]\n",
        "\n",
        "  # return the array\n",
        "  return inputs, outputs\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def prep_data(inputs, outputs):\n",
        "\n",
        "  # TRANSFORM DATE AND PRESSURE INPUTS\n",
        "\n",
        "  # method to help transform date\n",
        "  def date_to_nth_day(the_date):\n",
        "    date = pd.to_datetime(the_date)\n",
        "    new_year_day = pd.Timestamp(year=date.year, month=1, day=1)\n",
        "    day_of_the_year = (date - new_year_day).days + 1\n",
        "    return day_of_the_year\n",
        "\n",
        "  # loop through inputs and perform transformations\n",
        "  for input in inputs:\n",
        "    # adjust date\n",
        "    date = input[0]\n",
        "    frac_year = date_to_nth_day(date) / 365.0\n",
        "    input[0] = int(date[0:4]) + frac_year \n",
        "    \n",
        "  return inputs, outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEXBXjl3wdr4",
        "cellView": "form"
      },
      "source": [
        "#@title ### Load, clean, and preprocess data.\n",
        "\n",
        "# create arrays to store train inputs and labels\n",
        "train_input_arrays = []\n",
        "train_output_arrays = []\n",
        "\n",
        "# for each file in our directory clean and preprocess data\n",
        "print(\"Processing the following training files:\")\n",
        "for file in os.listdir(training_dir):\n",
        "  display(training_dir+file)\n",
        "\n",
        "  curr_inputs, curr_outputs = process_glider_file(training_dir+file)\n",
        "  curr_inputs, curr_outputs = prep_data(curr_inputs, curr_outputs)\n",
        "  train_input_arrays.append(curr_inputs)\n",
        "  train_output_arrays.append(curr_outputs)\n",
        "\n",
        "print(\"Complete\\n\")\n",
        "# combine all arrays of inputs and labels\n",
        "train_inputs = np.concatenate(train_input_arrays)\n",
        "train_outputs = np.concatenate(train_output_arrays)\n",
        "\n",
        "# convert all inputs and outputs to float type\n",
        "train_inputs = train_inputs.astype('float')\n",
        "train_outputs = train_outputs.astype('float')\n",
        "\n",
        "# if we're reducing density, reduce it\n",
        "if reduce_density:\n",
        "  indices = range(0,train_outputs.shape[0],density_reduction_factor)\n",
        "  train_inputs = np.take(train_inputs, indices, axis=0)\n",
        "  train_outputs = np.take(train_outputs, indices, axis=0)\n",
        "\n",
        "# shuffle training data\n",
        "train_inputs, train_outputs = shuffle(train_inputs, train_outputs, \n",
        "                                      random_state=101)\n",
        "\n",
        "# display some metadata about training inputs and labels\n",
        "print(\"Shape of training inputs: \" + str(train_inputs.shape))\n",
        "print(\"Shape of training outputs: \" + str(train_outputs.shape))\n",
        "\n",
        "num_print_rows = 5\n",
        "print(\"First {} inputs:\".format(num_print_rows))\n",
        "display(train_inputs[0:num_print_rows])\n",
        "print(\"First {} outputs:\".format(num_print_rows))\n",
        "display(train_outputs[0:num_print_rows])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTNlf0sIAiAU"
      },
      "source": [
        "## Create and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOQZcEtIv0bC",
        "cellView": "form"
      },
      "source": [
        "#@title ### Build and compile model.\n",
        "# METHOD TO BUILD AND COMPILE MODEL\n",
        "\n",
        "# name of the model to save\n",
        "model_name = model_name + \"_Layers({})\".format(model_layers)\n",
        "\n",
        "#create model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# create and add normalization layer\n",
        "normalizer = preprocessing.Normalization(axis=-1)\n",
        "normalizer.adapt(train_inputs)\n",
        "model.add(normalizer)\n",
        "\n",
        "# add hidden layers\n",
        "if len(act_funcs) == 0:\n",
        "  for layer in model_layers:\n",
        "    model.add(layers.Dense(layer, activation='sigmoid'))\n",
        "else:\n",
        "  for i in range(0, len(model_layers)):\n",
        "    model.add(layers.Dense(model_layers[i], activation=act_funcs[i]))\n",
        "\n",
        "# add final output layer\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=[ metrics.MeanAbsoluteError(),\n",
        "                        metrics.MeanSquaredError(),\n",
        "                        metrics.RootMeanSquaredError() ])\n",
        "\n",
        "# Create model and display summary\n",
        "print('Model Name: ' + model_name)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPJ-zZG4wBhg",
        "cellView": "form"
      },
      "source": [
        "#@title ### Train model.\n",
        "%%time\n",
        "history = model.fit(\n",
        "    train_inputs, train_outputs,\n",
        "    validation_split=0.1, epochs=100,\n",
        "    use_multiprocessing=True)\n",
        "\n",
        "model.save(model_dir+model_name)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss History')\n",
        "plt.ylabel('Loss (mean_squared_error)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbZLKaXfweiX",
        "cellView": "form"
      },
      "source": [
        "#@title # Visualize Accuracy on Training Set\n",
        "test_predictions = model.predict(train_inputs).flatten()\n",
        "\n",
        "ax = plt.axes(aspect='equal')\n",
        "\n",
        "plt.scatter(train_outputs, test_predictions)\n",
        "\n",
        "lims = [\n",
        "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
        "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
        "]\n",
        "plt.ylim(lims)\n",
        "plt.xlim(lims)\n",
        "plt.plot(lims,lims, color='black')\n",
        "\n",
        "plt.title('pH Predictions vs Observations')\n",
        "plt.xlabel('True Values pH')\n",
        "plt.ylabel('Predictions pH')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExhREZGq5sjN"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJV1rwiLitan",
        "cellView": "form"
      },
      "source": [
        "#@title # Get Testing Data\n",
        "test_input_arrays = []\n",
        "test_output_arrays = []\n",
        "\n",
        "for file in os.listdir(testing_dir):\n",
        "  display(testing_dir+file)\n",
        "\n",
        "  curr_inputs, curr_outputs = process_glider_file(testing_dir+file)\n",
        "  curr_inputs, curr_outputs = prep_data(curr_inputs, curr_outputs)\n",
        "  test_input_arrays.append(curr_inputs)\n",
        "  test_output_arrays.append(curr_outputs)\n",
        "\n",
        "test_inputs = np.concatenate(test_input_arrays)\n",
        "test_outputs = np.concatenate(test_output_arrays)\n",
        "\n",
        "test_inputs = test_inputs.astype('float')\n",
        "test_outputs = test_outputs.astype('float')\n",
        "\n",
        "display(test_inputs.shape)\n",
        "display(test_outputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf5ez5ZcayFi",
        "cellView": "form"
      },
      "source": [
        "#@title # Depth Conversion\n",
        "\n",
        "# FUNCTION: CONVERT PRESSURE TO DEPTH\n",
        "# Python version of the following MATLAB function\n",
        "'''\n",
        "% SW_DPTH    Depth from pressure\n",
        "%===========================================================================\n",
        "% SW_DPTH   $Id: sw_dpth.m,v 1.1 2003/12/12 04:23:22 pen078 Exp $\n",
        "%           Copyright (C) CSIRO, Phil Morgan 1992.\n",
        "%\n",
        "% USAGE:  dpth = sw_dpth(P,LAT)\n",
        "%\n",
        "% DESCRIPTION:\n",
        "%    Calculates depth in metres from pressure in dbars.\n",
        "%\n",
        "% INPUT:  (all must have same dimensions)\n",
        "%   P   = Pressure    [db]\n",
        "%   LAT = Latitude in decimal degress north [-90..+90]\n",
        "%         (lat may have dimensions 1x1 or 1xn where P(mxn).\n",
        "%\n",
        "% OUTPUT:\n",
        "%  dpth = depth [metres]\n",
        "%\n",
        "% AUTHOR:  Phil Morgan 92-04-06  (morgan@ml.csiro.au)\n",
        "%\n",
        "% DISCLAIMER:\n",
        "%   This software is provided \"as is\" without warranty of any kind.\n",
        "%   See the file sw_copy.m for conditions of use and licence.\n",
        "%\n",
        "% REFERENCES:\n",
        "%    Unesco 1983. Algorithms for computation of fundamental properties of\n",
        "%    seawater, 1983. _Unesco Tech. Pap. in Mar. Sci._, No. 44, 53 pp.\n",
        "%=========================================================================\n",
        "'''\n",
        "def pres_to_depth(pres, lat):\n",
        "\n",
        "  # define constants\n",
        "  DEG2RAD = math.pi/180\n",
        "  c1 = 9.72659\n",
        "  c2 = -2.2512E-5\n",
        "  c3 = 2.279E-10\n",
        "  c4 = -1.82E-15\n",
        "  gam_dash = 2.184E-6\n",
        "\n",
        "  # convert latitude\n",
        "  LAT = abs(lat)\n",
        "  X = math.sin(LAT*DEG2RAD)\n",
        "  X = X*X\n",
        "\n",
        "  # calculate denomenator and numerator\n",
        "  denom = 9.780318*(1.0+(5.2788E-3 + 2.36E-5*X)*X) + gam_dash*0.5*pres \n",
        "  numer = (((c4 * pres + c3) * pres + c2) * pres + c1) * pres\n",
        "\n",
        "  # return quotient\n",
        "  return numer / denom\n",
        "\n",
        "\n",
        "# FUNCTION: CONVERT ARRAY OF INPUTS TO AN ARRAY OF DEPTHS\n",
        "def get_depths(input_arr):\n",
        "  # create numpy array to store depths\n",
        "  depths = np.zeros(len(input_arr))\n",
        "  # loop through inputs and calculate depth\n",
        "  ind = 0\n",
        "  for input in input_arr:\n",
        "    depths[ind] = pres_to_depth(input[3], input[1])\n",
        "    ind += 1\n",
        "  # return depth array\n",
        "  return depths\n",
        "\n",
        "\n",
        "# get depths from inputs\n",
        "test_depths = get_depths(test_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPv5AWOla1_W",
        "cellView": "form"
      },
      "source": [
        "#@title # Make Predictions\n",
        "\n",
        "# make predictions on testing dataset with neural network\n",
        "test_predictions = model.predict(test_inputs, verbose=1).flatten()\n",
        "\n",
        "# GET DATAPOINTS FROM SHALLOW DEPTHS (<200m)\n",
        "# loop through depths and get indices of deep datapoints \n",
        "ind = 0\n",
        "inds = []\n",
        "for depth in test_depths:\n",
        "  if depth > shallow:\n",
        "    inds.append(ind)\n",
        "  ind += 1\n",
        "\n",
        "# delete deep datapoints and store resulting array of shallow datapoints\n",
        "shallow_test_depths = np.delete(test_depths, inds, axis=0)\n",
        "shallow_test_predictions = np.delete(test_predictions, inds, axis =0)\n",
        "shallow_outputs = np.delete(test_outputs, inds, axis=0)\n",
        "\n",
        "# ensure there are no deep datapoints\n",
        "count = 0\n",
        "for depth in shallow_test_depths:\n",
        "  if depth > shallow:\n",
        "    count += 1\n",
        "print('Number of datapoints of depth greater than {}m: {}'.format(shallow, count))\n",
        "\n",
        "print('Shape of each shallow data array (all should be equal):')\n",
        "display(shallow_test_depths.shape)\n",
        "display(shallow_test_predictions.shape)\n",
        "display(shallow_outputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksth7oAfd46B"
      },
      "source": [
        "## Error Metrics\n",
        "### MAE, MSE, RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug-hWLS29bru",
        "cellView": "form"
      },
      "source": [
        "#@title ### Error metrics table. \n",
        "# TEST INITIAL PERFORMANCE ON TRAINING, VALIDATION, & TESTING SETS\n",
        "\n",
        "model_metrics = np.zeros((1,6))\n",
        "\n",
        "# get metrics for DNN\n",
        "mae = mean_absolute_error(test_outputs, test_predictions)\n",
        "mse = mean_squared_error(test_outputs, test_predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "# round metrics to 5 decimal places\n",
        "model_metrics[0][0] = np.round(mae, 5)\n",
        "model_metrics[0][1] = np.round(mse, 5)\n",
        "model_metrics[0][2] = np.round(rmse, 5)\n",
        "# get metrics for DNN\n",
        "s_mae = mean_absolute_error(shallow_outputs, shallow_test_predictions)\n",
        "s_mse = mean_squared_error(shallow_outputs, shallow_test_predictions)\n",
        "s_rmse = math.sqrt(s_mse)\n",
        "# round metrics to 5 decimal places\n",
        "model_metrics[0][3] = np.round(s_mae, 5)\n",
        "model_metrics[0][4] = np.round(s_mse, 5)\n",
        "model_metrics[0][5] = np.round(s_rmse, 5)\n",
        "\n",
        "# create table\n",
        "# labels for columns and rows\n",
        "col_labels = ['MAE', 'MSE', 'RMSE', 'S-MAE*', 'S-MSE*', 'S-RMSE*']\n",
        "row_labels = [model_name]\n",
        "\n",
        "# add data to table\n",
        "fig, ax = plt.subplots(1, figsize=(10,3.5))\n",
        "table = ax.table(cellText=model_metrics, cellLoc='center', loc='center', \n",
        "                 rowLabels=row_labels, colLabels=col_labels)\n",
        "# title plot\n",
        "fig.suptitle('Error Metrics',\n",
        "             ha='center', va='center', fontsize=20, weight='bold')\n",
        "# scale plot\n",
        "table.set_fontsize(15)\n",
        "table.scale(1,4)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.text(1,0, \"* \\\"S-\\\" denotes \\\"Shallow\\\" which specifies depths less than 200m\",\n",
        "         ha='right', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# save and show plot\n",
        "if save_figs:\n",
        "  plt.savefig(fig_dir+'combined_err_metrics.png', bbox_inches='tight')\n",
        "if show_figs:\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK1lP-VCegLf"
      },
      "source": [
        "## Estimates vs Observations 1-1 Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI8Sxzo-wAvZ"
      },
      "source": [
        "plt.rcParams[\"axes.edgecolor\"] = \"white\"\n",
        "plt.rcParams[\"axes.linewidth\"]  = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8ZvtlCGViUu",
        "cellView": "form"
      },
      "source": [
        "#@title ### For overall dataset, plot predictions vs observations.\n",
        "\n",
        "fig, ax1 = plt.subplots(1, 1, figsize=(10,5), sharey=True)\n",
        "ax1.set_aspect('equal')\n",
        "\n",
        "# plot scatter plots of estimations vs observations\n",
        "# (x=observations, y=estimations, blue=our DNN, red=CANYON-B)\n",
        "ax1.scatter(test_outputs, test_predictions, label=model_name,\n",
        "            color=m_col, s=10, alpha=0.1)\n",
        "\n",
        "\n",
        "ax1.set_xlabel('pH Measured')\n",
        "ax1.xaxis.label.set_size(14)\n",
        "ax1.set_ylabel('pH Estimated')\n",
        "ax1.yaxis.label.set_size(14)\n",
        "\n",
        "# get axis limits from min and max ofdata\n",
        "lims = [\n",
        "    np.min([ax1.get_xlim(), ax1.get_ylim()]),  # min of both axes\n",
        "    np.max([ax1.get_xlim(), ax1.get_ylim()]),  # max of both axes\n",
        "]\n",
        "\n",
        "ax1.plot(lims,lims, color='k')\n",
        "ax1.set_ylim(lims)\n",
        "ax1.set_xlim(lims)\n",
        "\n",
        "# title, label, and legend plot\n",
        "fig.suptitle('pH Estimations vs Observations\\n', y=1,\n",
        "             va='center', fontsize=18, weight='bold')\n",
        "ax1.set_title(model_name, fontsize=16, weight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# show plot\n",
        "if save_figs:\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(fig_dir+'est_v_obs.png', bbox_inches='tight')\n",
        "if show_figs:\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av8KldCEbPmO",
        "cellView": "form"
      },
      "source": [
        "#@title ### For shallow dataset, plot predictions vs observations.\n",
        "fig, ax1 = plt.subplots(1, 1, figsize=(10,5), sharey=True)\n",
        "ax1.set_aspect('equal')\n",
        "\n",
        "# plot scatter plots of estimations vs observations\n",
        "# (x=observations, y=estimations, blue=our DNN, red=CANYON-B)\n",
        "ax1.scatter(shallow_outputs, shallow_test_predictions, label=model_name,\n",
        "            color=m_col, s=10, alpha=0.1)\n",
        "\n",
        "ax1.set_xlabel('pH Measured')\n",
        "ax1.xaxis.label.set_size(14)\n",
        "ax1.set_ylabel('pH Estimated')\n",
        "ax1.yaxis.label.set_size(14)\n",
        "\n",
        "# get axis limits from min and max ofdata\n",
        "lims = [\n",
        "    np.min([ax1.get_xlim(), ax1.get_ylim()]),  # min of both axes\n",
        "    np.max([ax1.get_xlim(), ax1.get_ylim()]),  # max of both axes\n",
        "]\n",
        "\n",
        "ax1.plot(lims,lims, color='black')\n",
        "ax1.set_ylim(lims)\n",
        "ax1.set_xlim(lims)\n",
        "\n",
        "# title, label, and legend plot\n",
        "fig.suptitle('pH Estimations vs Observations' + d_label, y=1.025,\n",
        "             va='center', fontsize=18, weight='bold')\n",
        "ax1.set_title(model_name, fontsize=16, weight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# show plot\n",
        "if save_figs:\n",
        "  plt.savefig(fig_dir+'est_v_obs_shallow.png', bbox_inches='tight')\n",
        "if show_figs:\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvaakTl9eUyX"
      },
      "source": [
        "## Depth vs Error Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzRh4tqtbYpM",
        "cellView": "form"
      },
      "source": [
        "#@title ### Plot error vs depth for overall dataset.\n",
        "# get errors (estimations - observations)\n",
        "error = test_predictions - test_outputs\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(10,11))\n",
        "\n",
        "# plot the scatter plots\n",
        "ax.scatter(error, test_depths, label=model_name, \n",
        "           color=m_col, s=10, alpha=0.1)\n",
        "\n",
        "# place a vertical line at x=0 to represent error=0\n",
        "ax.axvline(x=0, color='black')\n",
        "# invert y-axis to better represent depth\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('pH Error (Est - Obs)')\n",
        "ax.xaxis.label.set_size(14)\n",
        "ax.set_ylabel('Depth (m)')\n",
        "ax.yaxis.label.set_size(14)\n",
        "\n",
        "# label plot\n",
        "fig.suptitle('pH Error vs Depth', y=1.025,\n",
        "             fontsize=20, weight='bold')\n",
        "ax.set_title(model_name, fontsize=16, weight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# show plot\n",
        "if save_figs:\n",
        "  plt.savefig(fig_dir+'err_v_depth.png', bbox_inches='tight')\n",
        "if show_figs:\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ef9vcfgba9_",
        "cellView": "form"
      },
      "source": [
        "#@title ### Plot Error vs depth for shallow dataset.\n",
        "# get errors (estimations - observations)\n",
        "shallow_error = shallow_test_predictions - shallow_outputs\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# plot the scatter plots\n",
        "ax.scatter(shallow_error, shallow_test_depths, label=model_name, \n",
        "           color=m_col, s=10, alpha=0.1)\n",
        "\n",
        "# place a vertical line at x=0 to represent error=0\n",
        "ax.axvline(x=0, color='black')\n",
        "# invert y-axis to better represent depth\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('pH Error (Est - Obs)')\n",
        "ax.xaxis.label.set_size(14)\n",
        "ax.set_ylabel('Depth (m)')\n",
        "ax.yaxis.label.set_size(14)\n",
        "\n",
        "# label plot\n",
        "fig.suptitle('pH Error vs Depth' + d_label, y=1.025,\n",
        "             va='center', fontsize=20, weight='bold')\n",
        "ax.set_title(model_name, fontsize=16, weight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# save and show plot\n",
        "if save_figs:\n",
        "  plt.savefig(fig_dir+'err_v_depth_shallow.png', bbox_inches='tight')\n",
        "if show_figs:\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpzRrZYY7P-Z"
      },
      "source": [
        "## Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7izKP9aKcruT"
      },
      "source": [
        "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
        "plt.rcParams[\"axes.linewidth\"]  = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_vY62eKxOQW",
        "cellView": "form"
      },
      "source": [
        "#@title ### Plot heatmap for error vs depth for overall dataset.\n",
        "\n",
        "fig, ax1 = plt.subplots(1, figsize=(10,10))\n",
        "\n",
        "ranges = [[-0.1, 0.1],\n",
        "          [0, 1000]]\n",
        "\n",
        "hh1 = ax1.hist2d(error, test_depths, \n",
        "                 range=ranges, bins=(150,100), cmap=plt.cm.plasma)\n",
        "ax1.invert_yaxis()\n",
        "ax1.axvline(x=0, color='w')\n",
        "fig.colorbar(hh1[3], ax=ax1)\n",
        "\n",
        "fig.suptitle('Heatmap of Error vs Depth', y=1.025,\n",
        "             ha='center', va='center', fontsize=20, weight='bold')\n",
        "ax1.set_title(model_name, fontsize=18, weight='bold')\n",
        "\n",
        "ax1.set_xlabel('pH Error (Est - Obs)')\n",
        "ax1.xaxis.label.set_size(16)\n",
        "ax1.set_ylabel('Depth (m)')\n",
        "ax1.yaxis.label.set_size(16)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "if save_figs:\n",
        "  plt.savefig(fig_dir + 'err_v_depth_heatmap.png', bbox_inches='tight')\n",
        "if show_figs:\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iBHr--WncgI",
        "cellView": "form"
      },
      "source": [
        "#@title ### Plot heatmap for error vs depth for shallow dataset.\n",
        "fig, ax1 = plt.subplots(1, figsize=(10,10))\n",
        "\n",
        "ranges = [[-0.1, 0.1],\n",
        "          [0,shallow]]\n",
        "\n",
        "hh1 = ax1.hist2d(shallow_error, shallow_test_depths, \n",
        "                 range=ranges, bins=(150,100), cmap=plt.cm.plasma)\n",
        "ax1.invert_yaxis()\n",
        "ax1.axvline(x=0, color='w')\n",
        "fig.colorbar(hh1[3], ax=ax1)\n",
        "\n",
        "fig.suptitle('Heatmap of Error vs Depth' + d_label, y=1.025,\n",
        "             ha='center', va='center', fontsize=20, weight='bold')\n",
        "ax1.set_title(model_name, fontsize=18, weight='bold')\n",
        "\n",
        "ax1.set_xlabel('pH Error (Est - Obs)')\n",
        "ax1.set_ylabel('Depth (m)')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "if save_figs:\n",
        "  plt.savefig(fig_dir+'err_v_depth_heatmap_shallow.png', bbox_inches='tight')\n",
        "if show_figs:\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "yUgOg1JCdQmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Finish notebook.\n",
        "print(\"Notebook complete.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X4J_i8GsdSSu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}